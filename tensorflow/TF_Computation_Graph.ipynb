{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- requirement: small_data/cal_house.json.gz -->\n",
    "# Optimization with the Computation Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is supposed to be optimized for the mathematical operations common in machine learning. Let's benchmark the two linear regression classes from the [TF_Intro_TensorFlow](TF_Intro_TensorFlow.ipynb) notebook, one of which is based on NumPy and one of which is based on TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "## THIS OPTION ALLOWS TF TO GROW VRAM ON THE GPU\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   try:\n",
    "#     # Currently, memory growth needs to be the same across GPUs\n",
    "#     for gpu in gpus:\n",
    "#       tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#     print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#   except RuntimeError as e:\n",
    "#     # Memory growth must be set before GPUs have been initialized\n",
    "#     print(e)\n",
    "\n",
    "\n",
    "## THIS OPTION CONFIGURES THE VRAM AVAILABLE FOR TF UP-FRONT\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 4GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionNP():\n",
    "    def __init__(self, eta=.1):\n",
    "        self.W = 0\n",
    "        self.b = 0\n",
    "        self.eta = eta\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        return ((X * self.W + self.b - y) ** 2).mean()\n",
    "    \n",
    "    def _gradients(self, X, y):\n",
    "        return {'W': (2 * X * (X * self.W + self.b - y)).mean(),\n",
    "                'b': (2 * (X * self.W + self.b - y)).mean()}\n",
    "        \n",
    "    def fit(self, X, y, steps=10):\n",
    "        for _ in range(steps):\n",
    "            gradients = self._gradients(X, y)\n",
    "            self.W = self.W - self.eta * gradients['W']\n",
    "            self.b = self.b - self.eta * gradients['b']\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionTF():\n",
    "    def __init__(self, eta=.1):\n",
    "        self.W = tf.Variable(0.)\n",
    "        self.b = tf.Variable(0.)\n",
    "        self.opt = tf.keras.optimizers.SGD(learning_rate=eta)\n",
    "    \n",
    "    def loss(self, X, y, return_func=False):\n",
    "        def loss_():\n",
    "            return tf.reduce_mean(tf.square(X * self.W + self.b - y))\n",
    "        \n",
    "        if not return_func:\n",
    "            return loss_()\n",
    "        \n",
    "        return loss_\n",
    "    \n",
    "    def fit(self, X, y, steps=10):\n",
    "        for _ in range(steps):\n",
    "            self.opt.minimize(self.loss(X, y, return_func=True), [self.W, self.b])\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "np_model = LinearRegressionNP()\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    tf_model = LinearRegressionTF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "with gzip.open(\"small_data/cal_house.json.gz\", \"r\") as fin:\n",
    "    housing = json.load(fin)\n",
    "    \n",
    "for train, test in ShuffleSplit(1, 0.2, random_state=42).split(housing['data']):\n",
    "    X_train = np.array(housing['data'])[train].astype(np.float32)\n",
    "    y_train = np.array(housing['target'])[train].astype(np.float32)\n",
    "    X_test = np.array(housing['data'])[test].astype(np.float32)\n",
    "    y_test = np.array(housing['target'])[test].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leeekwon/miniconda3/envs/ds37/lib/python3.7/site-packages/numpy/core/_methods.py:151: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "/home/leeekwon/miniconda3/envs/ds37/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.07 s ± 41 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "np_model.fit(X_train[:, 0:1], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Square in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Mean in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Tile in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RealDiv in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Shape in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op BroadcastGradientArgs in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Sum in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ResourceApplyGradientDescent in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignAddVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "7.99 s ± 37.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with tf.device('/CPU:0'):\n",
    "    tf_model.fit(X_train[:, 0:1], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NumPy model is faster (slightly) than the TensorFlow model! What's going on here?\n",
    "\n",
    "TensorFlow's computational advantage comes in two forms:\n",
    "\n",
    "1. Making use of a computational graph. The graph is compiled, allowing for fast execution.\n",
    "2. TensorFlow can place computations on hardware accelerators (GPUs, TPUs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow can record tensors and operations on them in a data structure called a computation graph. This graph can be compiled before runtime for faster execution (e.g. operations can be parallelized based on analysis of the graph during compilation). The graph also enables TensorFlow to compute gradients more directly since the results of operations don't have to be accumulated as they are executed. \n",
    "\n",
    "In TensorFlow 2.0 we mark which computations should be entered into the graph using the `tf.function` decorator. In TensorFlow 1.x, all computation was accomplished through the computation graph. To contrast these two approaches, the graph generated by `tf.function` is sometimes referred to as the [autograph](https://www.tensorflow.org/beta/guide/autograph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Square in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mean in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Tile in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RealDiv in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Neg in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Shape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op BroadcastGradientArgs in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sum in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_gradients_fn_3585 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Equal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op All in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "def mse(W, b, X, y):\n",
    "    return tf.reduce_mean(tf.square(X * W + b - y))\n",
    "    \n",
    "def gradients(W, b, X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = mse(W, b, X, y)\n",
    "        \n",
    "    return tape.gradient(loss, [W, b])\n",
    "\n",
    "@tf.function\n",
    "def gradients_fn(W, b, X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = mse(W, b, X, y)\n",
    "        \n",
    "    return tape.gradient(loss, [W, b])\n",
    "\n",
    "X = tf.random.uniform((10000, 1))\n",
    "y = tf.random.uniform((10000, 1))\n",
    "W = tf.Variable(0.)\n",
    "b = tf.Variable(0.)\n",
    "\n",
    "assert tf.math.reduce_all(tf.equal(gradients(W, b, X, y), gradients_fn(W, b, X, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.29 ms ± 24.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "gradients(W, b, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281 µs ± 15.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "gradients_fn(W, b, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the graph is compiled at the time of the first function call (known as just-in-time, JIT, compilation). We only realize faster computation upon subsequent function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op __inference_gradients_fn_287518 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Time elapsed on first call: 0.08303952217102051\n",
      "Time elapsed on second call: 0.0008637905120849609\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "@tf.function\n",
    "def gradients_fn(W, b, X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = mse(W, b, X, y)\n",
    "        \n",
    "    return tape.gradient(loss, [W, b])\n",
    "\n",
    "start = time.time()\n",
    "gradients_fn(W, b, X, y)\n",
    "print(\"Time elapsed on first call: {}\".format(time.time() - start))\n",
    "\n",
    "start = time.time()\n",
    "gradients_fn(W, b, X, y)\n",
    "print(\"Time elapsed on second call: {}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, compilation takes place any time the arguments different in `shape` or `dtype` from the previous arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op StridedSlice in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_gradients_fn_287570 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Time elapsed on first call: 0.03322243690490723\n",
      "Time elapsed on second call: 0.0010814666748046875\n"
     ]
    }
   ],
   "source": [
    "i = 100\n",
    "\n",
    "start = time.time()\n",
    "gradients_fn(W, b, X[:i], y[:i])\n",
    "print(\"Time elapsed on first call: {}\".format(time.time() - start))\n",
    "\n",
    "start = time.time()\n",
    "gradients_fn(W, b, X[:i], y[:i])\n",
    "print(\"Time elapsed on second call: {}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we typically want to supply a `tf.function` with tensors (or NumPy arrays) as arguments. Python native types do not have `shape` or `dtype` information, and therefore the graph will be recompiled any time the values change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op __inference_gradients_fn_287623 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Time elapsed on first call: 0.7134580612182617\n",
      "Executing op __inference_gradients_fn_287665 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Time elapsed on second call: 0.6606919765472412\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def gradients_fn(W, b, X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = mse(W, b, X, y)\n",
    "        \n",
    "    return tape.gradient(loss, [W, b])\n",
    "\n",
    "X_np = np.random.random((10000, 1)).astype(np.float32).tolist()\n",
    "y_np = np.random.random((10000, 1)).astype(np.float32).tolist()\n",
    "\n",
    "start = time.time()\n",
    "gradients_fn(W, b, X_np, y_np)\n",
    "print(\"Time elapsed on first call: {}\".format(time.time() - start))\n",
    "\n",
    "# UNCOMMENT TO CONTRAST\n",
    "X_np = np.random.random((10000, 1)).astype(np.float32).tolist()\n",
    "y_np = np.random.random((10000, 1)).astype(np.float32).tolist()\n",
    "\n",
    "start = time.time()\n",
    "gradients_fn(W, b, X_np, y_np)\n",
    "print(\"Time elapsed on second call: {}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize `LinearRegressionTF`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `tf.function` to optimize `LinearRegressionTF`. Demonstrate that the `LinearRegressionTF.fit` runs faster than `LinearRegressionNP.fit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using accelerators (GPUs/TPUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making use of GPUs requires additional drivers and libraries that are not packaged with TensorFlow by default. Instead of installing `tensorflow` (which is installed in this environment) with your package manager, you would install `tensorflow-gpu`. Furthermore, your machine must have the compatible hardware, which you can check in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow built with GPU support: True\n",
      "Compatible GPU installed: True\n"
     ]
    }
   ],
   "source": [
    "print('TensorFlow built with GPU support: {}'.format(tf.test.is_built_with_cuda()))\n",
    "print('Compatible GPU installed: {}'.format(tf.test.is_gpu_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tensor and operation has a device attribute that describes what hardware is responsible for its evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:GPU:0'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow will automatically preferentially place operations with GPU implementations on GPUs. If you need to exercise more manual control, TensorFlow provides a context manager for controlling on what device a tensor or operation resides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:CPU:0'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    on_cpu = tf.constant([[1, 2,], [3, 4]])\n",
    "    \n",
    "on_cpu.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately due to limitations of the environment, we only have one device available for demonstrating this facet of TensorFlow, but you can [read more about this capability in the TensorFlow documentation](https://www.tensorflow.org/beta/guide/using_gpu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 Pragmatic Institute. This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
